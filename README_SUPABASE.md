# Supabase Link Tracking for Crawl Pipeline

This document explains how to configure Supabase to track crawled links and integrate it with `scripts/run_links_crawl.py` so links are skipped if already processed.

## Overview
- The run script uses Supabase (optional) to check whether a link has been crawled/ingested.
- Before crawling a link, the script queries a Supabase table by `url`.
- If a row exists with `status` in {`crawled`, `ingested`}, the link is skipped.
- After processing a link, the script upserts a record with status `pending`, `crawled`, `failed`, or `ingested` and stores output filenames.
- If Supabase is not configured, the script runs normally without link tracking.

## Requirements
- Supabase Python client (install via pip):
  ```bash
  pip install supabase
  ```
- Supabase project (URL and anon/public key).

## Environment Variables
Set these in your shell or `.env` (the project already loads `.env`):

- `SUPABASE_URL`: Your Supabase project URL (e.g., `https://xyzcompany.supabase.co`).
- `SUPABASE_ANON_KEY`: The anon/public key from Supabase.
- `SUPABASE_LINKS_TABLE` (optional): Table name to use. Default: `crawl_links`.

Example `.env` entries:
```
SUPABASE_URL=https://your-project.supabase.co
SUPABASE_ANON_KEY=your-anon-key
SUPABASE_LINKS_TABLE=crawl_links
```

## Table Schema
Create a table named `crawl_links` with these columns:

- `id`: bigint generated or uuid (primary key)
- `url`: text (unique) — unique index required
- `status`: text — values like `pending`, `crawled`, `failed`, `ingested`
- `note`: text — short message (e.g., `ok`, `ingest_failed`, `no_json`)
- `output_files`: text — JSON string array of produced file paths
- `last_crawled_at`: timestamptz — timestamp of last crawl

SQL example:
```sql
create table if not exists public.crawl_links (
  id bigint generated by default as identity primary key,
  url text unique not null,
  status text not null default 'pending',
  note text,
  output_files text,
  last_crawled_at timestamptz
);
create unique index if not exists crawl_links_url_idx on public.crawl_links(url);
```

## Usage
Run the link crawl (with Supabase tracking, if configured):
```bash
python scripts/run_links_crawl.py --wiki-dir ~/wiki --max-pages 10 --retries 3 --retry-delay 5
```

The script will:
- Extract links via `scripts/extract_category_links.py`.
- For each link:
  - Check Supabase for existing `url` entry; skip if already `crawled` or `ingested`.
  - Upsert `pending` when queued.
  - Crawl and save JSON to `data/crawl`.
  - Convert JSON to one or more `.txt` files.
  - Upload `.txt` into Qdrant via `scripts/ingest.py`.
  - Upsert `ingested` with produced file paths; record failures with `failed` and a descriptive `note`.

## Notes
- If Supabase env vars are missing or the client fails to initialize, the script continues without tracking.
- Deduplication for text uploads is handled by content-hash index in `scripts/ingest.py`. It stores an index at `data/crawl/ingested_index.json` to avoid re-ingesting identical content.
- If remote VNPT embeddings or Qdrant are unavailable, ingestion will fall back to local embeddings and local Qdrant storage when configured.

## Troubleshooting
- Ensure `SUPABASE_URL` and `SUPABASE_ANON_KEY` are correct and accessible.
- If you see DNS errors when ingesting (e.g., VNPT API), set `USE_VNPT_API=false` and unset `QDRANT_URL` to force local ingestion:
  ```bash
  export USE_VNPT_API=false
  unset QDRANT_URL
  unset QDRANT_API_KEY
  ```
- If ingestion fails, check `data/crawl/failed_ingest.txt` for files to retry later.

## Security
- The anon key allows public table reads/writes according to your Row Level Security (RLS) rules. Configure RLS appropriately for the `crawl_links` table.
- For production, prefer service role keys and secure environments; update the client creation accordingly.

